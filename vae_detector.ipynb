{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e658f3ec",
   "metadata": {},
   "source": [
    "# 跑冒滴漏异常检测（条件VAE-UNet）\n",
    "\n",
    "- 数据来自课程要求的 DATASET2（train 2836 + test 120），标签使用 YOLO 格式，仅保留前三类（Oil_accumulation / Oil_seepage / Standing_water）。\n",
    "- 参考 PPT 提示，本文实现了一个条件 VAE 版本的 UNet，用生成式建模的方式学习异常区域，再通过后处理还原成提交所需的检测框。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4217f63",
   "metadata": {},
   "source": [
    "## 流程概览\n",
    "1. 解析 train 标签并构建热力图 mask；划分 train/val。\n",
    "2. 构建条件 VAE-UNet（image encoder + latent branch + decoder）并以 mask 重建损失训练。\n",
    "3. 使用验证集监控 IoU，保存最优 checkpoint。\n",
    "4. 推理阶段得到每类概率图，阈值+NMS 转为 YOLO 坐标，生成 120 个结果文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "350bffc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "train images: 2836, test images: 120\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib; matplotlib.use('Agg');\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "DATA_ROOT = Path('d:/BaiduNetdiskDownload/dataset_work_final')\n",
    "TRAIN_IMG_DIR = DATA_ROOT / 'images' / 'train'\n",
    "TEST_IMG_DIR = DATA_ROOT / 'images' / 'test'\n",
    "LABEL_DIR = DATA_ROOT / 'labels' / 'train'\n",
    "CHECKPOINT_DIR = DATA_ROOT / 'artifacts'\n",
    "RESULT_DIR = DATA_ROOT / 'results_cvae'\n",
    "\n",
    "for path in [CHECKPOINT_DIR, RESULT_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CLASS_NAMES = ['Oil_accumulation', 'Oil_seepage', 'Standing_water']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "IMAGE_SIZE = (288, 512)  # (H, W), divisible by 16 for UNet\n",
    "SEED = 7\n",
    "FAST_DEBUG = False  # quick smoke test; set True for mini-run\n",
    "FAST_TRAIN_LIMIT = 256\n",
    "FAST_VAL_LIMIT = 64\n",
    "VAL_FRACTION = 0.1\n",
    "BASE_BATCH_SIZE = 2\n",
    "BASE_EPOCHS = 5\n",
    "BATCH_SIZE = 1 if FAST_DEBUG else BASE_BATCH_SIZE\n",
    "EPOCHS = 1 if FAST_DEBUG else BASE_EPOCHS\n",
    "LEARNING_RATE = 3e-4\n",
    "LATENT_DIM = 64\n",
    "KL_WEIGHT = 1e-4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "IS_WINDOWS = os.name == 'nt'\n",
    "MAX_WORKERS = max(0, min(4, (os.cpu_count() or 0) // 2))\n",
    "NUM_WORKERS = 0 if FAST_DEBUG or IS_WINDOWS else MAX_WORKERS\n",
    "PREFETCH_FACTOR = 2\n",
    "PIN_MEMORY = DEVICE == 'cuda'\n",
    "\n",
    "def seed_everything(seed: int = 7):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "print(f'device: {DEVICE}')\n",
    "print(f'train images: {len(list(TRAIN_IMG_DIR.glob(\"*.jpg\")))}, test images: {len(list(TEST_IMG_DIR.glob(\"*.jpg\")))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8ce9805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_boxes(label_path: Path):\n",
    "    boxes = []\n",
    "    if not label_path.exists():\n",
    "        return boxes\n",
    "    with label_path.open() as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            cls = int(float(parts[0]))\n",
    "            if cls >= NUM_CLASSES:\n",
    "                continue\n",
    "            cx, cy, w, h = map(float, parts[1:])\n",
    "            boxes.append((cls, cx, cy, w, h))\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def boxes_to_mask(boxes, out_h, out_w, num_classes=NUM_CLASSES):\n",
    "    mask = np.zeros((out_h, out_w, num_classes), dtype=np.float32)\n",
    "    for cls, cx, cy, bw, bh in boxes:\n",
    "        if cls >= num_classes:\n",
    "            continue\n",
    "        x1 = (cx - bw / 2.0) * out_w\n",
    "        x2 = (cx + bw / 2.0) * out_w\n",
    "        y1 = (cy - bh / 2.0) * out_h\n",
    "        y2 = (cy + bh / 2.0) * out_h\n",
    "        x1 = np.clip(x1, 0, out_w - 1)\n",
    "        x2 = np.clip(x2, 0, out_w - 1)\n",
    "        y1 = np.clip(y1, 0, out_h - 1)\n",
    "        y2 = np.clip(y2, 0, out_h - 1)\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "        x1i, x2i = int(np.floor(x1)), int(np.ceil(x2))\n",
    "        y1i, y2i = int(np.floor(y1)), int(np.ceil(y2))\n",
    "        mask[y1i:y2i, x1i:x2i, cls] = 1.0\n",
    "    return mask\n",
    "\n",
    "\n",
    "def apply_augmentations(image, mask):\n",
    "    if random.random() < 0.5:\n",
    "        image = image[:, ::-1].copy()\n",
    "        mask = mask[:, ::-1].copy()\n",
    "    if random.random() < 0.3:\n",
    "        alpha = 1.0 + 0.3 * (random.random() * 2 - 1)\n",
    "        beta = 15.0 * (random.random() * 2 - 1)\n",
    "        image = np.clip(image * alpha + beta, 0, 255).astype(np.float32)\n",
    "    if random.random() < 0.2:\n",
    "        noise = np.random.normal(0, 5, size=image.shape).astype(np.float32)\n",
    "        image = np.clip(image + noise, 0, 255)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def draw_boxes(image, boxes):\n",
    "    vis = image.copy()\n",
    "    h, w = vis.shape[:2]\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "    for cls, cx, cy, bw, bh in boxes:\n",
    "        x1 = int((cx - bw / 2) * w)\n",
    "        y1 = int((cy - bh / 2) * h)\n",
    "        x2 = int((cx + bw / 2) * w)\n",
    "        y2 = int((cy + bh / 2) * h)\n",
    "        color = colors[cls % len(colors)]\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(vis, CLASS_NAMES[cls], (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n",
    "    return vis\n",
    "\n",
    "\n",
    "def mask_overlay(image_tensor, mask_tensor):\n",
    "    image = image_tensor.numpy().transpose(1, 2, 0)\n",
    "    image = np.clip(image * 255.0, 0, 255).astype(np.uint8)\n",
    "    mask = mask_tensor.numpy().transpose(1, 2, 0)\n",
    "    colors = np.array([(255, 0, 0), (0, 255, 0), (0, 0, 255)], dtype=np.float32)\n",
    "    overlay = image.copy().astype(np.float32)\n",
    "    for idx in range(min(mask.shape[-1], len(colors))):\n",
    "        cls_mask = mask[..., idx] > 0.5\n",
    "        if cls_mask.sum() == 0:\n",
    "            continue\n",
    "        overlay[cls_mask] = 0.6 * overlay[cls_mask] + 0.4 * colors[idx]\n",
    "    return overlay.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1951410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FAST_DEBUG] using 256/2553 train and 64/283 val samples\n",
      "train samples: 256, val samples: 64\n"
     ]
    }
   ],
   "source": [
    "all_image_ids = sorted([p.stem for p in TRAIN_IMG_DIR.glob('*.jpg')])\n",
    "random.Random(SEED).shuffle(all_image_ids)\n",
    "val_count = max(1, int(len(all_image_ids) * VAL_FRACTION))\n",
    "val_ids = sorted(all_image_ids[:val_count])\n",
    "train_ids = sorted(all_image_ids[val_count:])\n",
    "\n",
    "if FAST_DEBUG:\n",
    "    original_train = len(train_ids)\n",
    "    original_val = len(val_ids)\n",
    "    train_ids = train_ids[:min(original_train, FAST_TRAIN_LIMIT)]\n",
    "    val_ids = val_ids[:min(original_val, FAST_VAL_LIMIT)]\n",
    "    print(f'[FAST_DEBUG] using {len(train_ids)}/{original_train} train and {len(val_ids)}/{original_val} val samples')\n",
    "\n",
    "print(f'train samples: {len(train_ids)}, val samples: {len(val_ids)}')\n",
    "with (CHECKPOINT_DIR / 'split.json').open('w', encoding='utf-8') as f:\n",
    "    json.dump({'train_ids': train_ids, 'val_ids': val_ids}, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ee988a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakDataset(Dataset):\n",
    "    def __init__(self, image_ids, image_dir, label_dir, image_size=IMAGE_SIZE, augment=False):\n",
    "        self.image_ids = image_ids\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = self.image_dir / f'{image_id}.jpg'\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.image_size[1], self.image_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        boxes = load_yolo_boxes(self.label_dir / f'{image_id}.txt')\n",
    "        mask = boxes_to_mask(boxes, self.image_size[0], self.image_size[1])\n",
    "\n",
    "        if self.augment:\n",
    "            image, mask = apply_augmentations(image, mask)\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        mask = mask.astype(np.float32)\n",
    "        image = torch.from_numpy(image.transpose(2, 0, 1))\n",
    "        mask = torch.from_numpy(mask.transpose(2, 0, 1))\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class LeakInferenceDataset(Dataset):\n",
    "    def __init__(self, image_paths, image_size=IMAGE_SIZE):\n",
    "        self.image_paths = sorted(image_paths)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        image = cv2.imread(str(path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        orig_h, orig_w = image.shape[:2]\n",
    "        image_resized = cv2.resize(image, (self.image_size[1], self.image_size[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        tensor = torch.from_numpy(image_resized.transpose(2, 0, 1)).float() / 255.0\n",
    "        return tensor, path.stem, (orig_h, orig_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b213b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6604\\3369542872.py:9: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "preview_ds = LeakDataset(train_ids[:4], TRAIN_IMG_DIR, LABEL_DIR, augment=False)\n",
    "fig, axes = plt.subplots(1, len(preview_ds), figsize=(18, 4))\n",
    "for ax, sample in zip(axes, preview_ds):\n",
    "    img, mask = sample\n",
    "    overlay = mask_overlay(img, mask)\n",
    "    ax.imshow(overlay)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('训练样本可视化（mask 覆盖在缩放后图片上）')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75483001",
   "metadata": {},
   "source": [
    "## 条件 VAE-UNet 结构\n",
    "- `image encoder` 提取逐级特征；`posterior encoder` 在训练时接收 (image, mask) 以推断潜变量；`prior encoder` 仅依赖 image。\n",
    "- 将采样得到的 latent `z` 拼接到 bottleneck，再经过带 skip 的 decoder 输出每类热力图。\n",
    "- 损失函数 = BCE + Dice + β·KL。β 设为 1e-4，用于稳定训练又保持生成式约束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9a86bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            ConvBlock(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = ConvBlock(out_ch + skip_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        diff_y = skip.size(2) - x.size(2)\n",
    "        diff_x = skip.size(3) - x.size(3)\n",
    "        if diff_y != 0 or diff_x != 0:\n",
    "            x = F.pad(x, [diff_x // 2, diff_x - diff_x // 2, diff_y // 2, diff_y - diff_y // 2])\n",
    "        x = torch.cat([skip, x], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_ch, base_ch):\n",
    "        super().__init__()\n",
    "        self.inc = ConvBlock(in_ch, base_ch)\n",
    "        self.down1 = DownBlock(base_ch, base_ch * 2)\n",
    "        self.down2 = DownBlock(base_ch * 2, base_ch * 4)\n",
    "        self.down3 = DownBlock(base_ch * 4, base_ch * 8)\n",
    "        self.down4 = DownBlock(base_ch * 8, base_ch * 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        return [x1, x2, x3, x4, x5]\n",
    "\n",
    "\n",
    "class PosteriorEncoder(nn.Module):\n",
    "    def __init__(self, in_ch, base_ch):\n",
    "        super().__init__()\n",
    "        self.enc = Encoder(in_ch, base_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)[-1]\n",
    "\n",
    "\n",
    "class LatentHead(nn.Module):\n",
    "    def __init__(self, in_ch, latent_dim):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.mu = nn.Linear(in_ch, latent_dim)\n",
    "        self.logvar = nn.Linear(in_ch, latent_dim)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        pooled = self.pool(feat).view(feat.size(0), -1)\n",
    "        return self.mu(pooled), self.logvar(pooled)\n",
    "\n",
    "\n",
    "class ConditionalVAEUNet(nn.Module):\n",
    "    def __init__(self, in_ch=3, num_classes=NUM_CLASSES, base_ch=32, latent_dim=LATENT_DIM):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(in_ch, base_ch)\n",
    "        self.posterior_encoder = PosteriorEncoder(in_ch + num_classes, base_ch)\n",
    "        bottleneck_ch = base_ch * 16\n",
    "        self.prior_head = LatentHead(bottleneck_ch, latent_dim)\n",
    "        self.posterior_head = LatentHead(bottleneck_ch, latent_dim)\n",
    "        self.up1 = UpBlock(bottleneck_ch + latent_dim, base_ch * 8, base_ch * 8)\n",
    "        self.up2 = UpBlock(base_ch * 8, base_ch * 4, base_ch * 4)\n",
    "        self.up3 = UpBlock(base_ch * 4, base_ch * 2, base_ch * 2)\n",
    "        self.up4 = UpBlock(base_ch * 2, base_ch, base_ch)\n",
    "        self.out_conv = nn.Conv2d(base_ch, num_classes, kernel_size=1)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, feats, z):\n",
    "        x1, x2, x3, x4, x5 = feats\n",
    "        b, _, h, w = x5.shape\n",
    "        z_map = z.view(b, self.latent_dim, 1, 1).expand(-1, -1, h, w)\n",
    "        x = torch.cat([x5, z_map], dim=1)\n",
    "        x = self.up1(x, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        return self.out_conv(x)\n",
    "\n",
    "    def forward(self, x, target_mask=None):\n",
    "        feats = self.encoder(x)\n",
    "        bottleneck = feats[-1]\n",
    "        prior_mu, prior_logvar = self.prior_head(bottleneck)\n",
    "        if self.training and target_mask is not None:\n",
    "            posterior_input = torch.cat([x, target_mask], dim=1)\n",
    "            posterior_feat = self.posterior_encoder(posterior_input)\n",
    "            post_mu, post_logvar = self.posterior_head(posterior_feat)\n",
    "            z = self.reparameterize(post_mu, post_logvar)\n",
    "            kl = 0.5 * torch.sum(\n",
    "                torch.exp(post_logvar - prior_logvar)\n",
    "                + ((prior_mu - post_mu) ** 2) / torch.exp(prior_logvar)\n",
    "                - 1.0 + (prior_logvar - post_logvar),\n",
    "                dim=1\n",
    "            )\n",
    "            kl = kl.mean()\n",
    "        else:\n",
    "            z = self.reparameterize(prior_mu, prior_logvar)\n",
    "            kl = torch.tensor(0.0, device=x.device)\n",
    "        logits = self.decode(feats, z)\n",
    "        return logits, kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e421a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(logits, targets, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    dims = (0, 2, 3)\n",
    "    intersection = (probs * targets).sum(dim=dims)\n",
    "    denom = probs.sum(dim=dims) + targets.sum(dim=dims)\n",
    "    dice = (2 * intersection + eps) / (denom + eps)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "\n",
    "def compute_iou(logits, targets, threshold=0.5, eps=1e-6):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > threshold).float()\n",
    "    dims = (0, 2, 3)\n",
    "    intersection = (preds * targets).sum(dim=dims)\n",
    "    union = preds.sum(dim=dims) + targets.sum(dim=dims) - intersection\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler):\n",
    "    model.train()\n",
    "    running = {'loss': 0.0, 'bce': 0.0, 'dice': 0.0, 'kl': 0.0}\n",
    "    for images, masks in tqdm(loader, total=len(loader)):\n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == 'cuda')):\n",
    "            logits, kl = model(images, masks)\n",
    "            bce = F.binary_cross_entropy_with_logits(logits, masks)\n",
    "            dice = dice_loss(logits, masks)\n",
    "            loss = bce + dice + KL_WEIGHT * kl\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running['loss'] += loss.item()\n",
    "        running['bce'] += bce.item()\n",
    "        running['dice'] += dice.item()\n",
    "        running['kl'] += kl.item()\n",
    "    for k in running:\n",
    "        running[k] /= max(1, len(loader))\n",
    "    return running\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    metrics = {'loss': 0.0, 'bce': 0.0, 'dice': 0.0, 'iou': 0.0}\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "            logits, _ = model(images, masks)\n",
    "            bce = F.binary_cross_entropy_with_logits(logits, masks)\n",
    "            dice = dice_loss(logits, masks)\n",
    "            loss = bce + dice\n",
    "            metrics['loss'] += loss.item()\n",
    "            metrics['bce'] += bce.item()\n",
    "            metrics['dice'] += dice.item()\n",
    "            metrics['iou'] += compute_iou(logits, masks)\n",
    "    for k in metrics:\n",
    "        metrics[k] /= max(1, len(loader))\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a047cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 256, val batches: 64\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LeakDataset(train_ids, TRAIN_IMG_DIR, LABEL_DIR, augment=True)\n",
    "val_dataset = LeakDataset(val_ids, TRAIN_IMG_DIR, LABEL_DIR, augment=False)\n",
    "loader_kwargs = {\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "}\n",
    "if NUM_WORKERS > 0:\n",
    "    loader_kwargs['persistent_workers'] = True\n",
    "    loader_kwargs['prefetch_factor'] = PREFETCH_FACTOR\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, **loader_kwargs)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, **loader_kwargs)\n",
    "print(f'train batches: {len(train_loader)}, val batches: {len(val_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f79376c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6604\\3138218722.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == 'cuda'))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b24e84d32e649f3ba03c3346826df51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6604\\1592437643.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE == 'cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'train_loss': 1.4616852323524654, 'train_dice': 0.016534110298380256, 'val_loss': 1.38950889185071, 'val_dice': 0.020868970081210136, 'val_iou': 0.08487947774271319, 'lr': 0.0}\n",
      ">> saved new best model to d:\\BaiduNetdiskDownload\\dataset_work_final\\artifacts\\cvae_detector.pt, IoU=0.0849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'train_loss': 1.4616852323524654,\n",
       "  'train_dice': 0.016534110298380256,\n",
       "  'val_loss': 1.38950889185071,\n",
       "  'val_dice': 0.020868970081210136,\n",
       "  'val_iou': 0.08487947774271319,\n",
       "  'lr': 0.0}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConditionalVAEUNet().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == 'cuda'))\n",
    "\n",
    "history = []\n",
    "best_iou = 0.0\n",
    "best_path = CHECKPOINT_DIR / 'cvae_detector.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    train_stats = train_one_epoch(model, train_loader, optimizer, scaler)\n",
    "    val_stats = evaluate(model, val_loader)\n",
    "    scheduler.step()\n",
    "    record = {\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_stats['loss'],\n",
    "        'train_dice': 1 - train_stats['dice'],\n",
    "        'val_loss': val_stats['loss'],\n",
    "        'val_dice': 1 - val_stats['dice'],\n",
    "        'val_iou': val_stats['iou'],\n",
    "        'lr': scheduler.get_last_lr()[0],\n",
    "    }\n",
    "    history.append(record)\n",
    "    print(record)\n",
    "    if record['val_iou'] > best_iou:\n",
    "        best_iou = record['val_iou']\n",
    "        torch.save({'model_state': model.state_dict(), 'config': {'image_size': IMAGE_SIZE, 'latent_dim': LATENT_DIM}}, best_path)\n",
    "        print(f'>> saved new best model to {best_path}, IoU={best_iou:.4f}')\n",
    "\n",
    "history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2900fcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6604\\1319989891.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "if history:\n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    val_iou = [h['val_iou'] for h in history]\n",
    "    train_loss = [h['train_loss'] for h in history]\n",
    "    val_loss = [h['val_loss'] for h in history]\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axes[0].plot(epochs, train_loss, label='train loss')\n",
    "    axes[0].plot(epochs, val_loss, label='val loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title('Loss curves')\n",
    "    axes[1].plot(epochs, val_iou, label='val IoU', color='purple')\n",
    "    axes[1].set_title('Validation IoU')\n",
    "    axes[1].legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18d94ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6604\\294766460.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(best_checkpoint, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from d:\\BaiduNetdiskDownload\\dataset_work_final\\artifacts\\cvae_detector.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6604\\294766460.py:26: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "best_checkpoint = CHECKPOINT_DIR / 'cvae_detector.pt'\n",
    "assert best_checkpoint.exists(), '请先完成训练再继续'\n",
    "state = torch.load(best_checkpoint, map_location=DEVICE)\n",
    "model.load_state_dict(state['model_state'])\n",
    "model.eval()\n",
    "print('Loaded checkpoint from', best_checkpoint)\n",
    "\n",
    "val_samples = min(4, len(val_dataset))\n",
    "fig, axes = plt.subplots(val_samples, 3, figsize=(12, 3 * val_samples))\n",
    "for row in range(val_samples):\n",
    "    img, mask = val_dataset[row]\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(img.unsqueeze(0).to(DEVICE), mask.unsqueeze(0).to(DEVICE))\n",
    "        probs = torch.sigmoid(logits)[0].cpu()\n",
    "    overlay_gt = mask_overlay(img, mask)\n",
    "    overlay_pred = mask_overlay(img, probs)\n",
    "    axes[row, 0].imshow(img.numpy().transpose(1, 2, 0))\n",
    "    axes[row, 0].set_title('Input')\n",
    "    axes[row, 1].imshow(overlay_gt)\n",
    "    axes[row, 1].set_title('GT mask')\n",
    "    axes[row, 2].imshow(overlay_pred)\n",
    "    axes[row, 2].set_title('Pred mask')\n",
    "    for col in range(3):\n",
    "        axes[row, col].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f296e8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, iou_thresh=0.4):\n",
    "    if not boxes:\n",
    "        return []\n",
    "    boxes = sorted(boxes, key=lambda x: x[1], reverse=True)\n",
    "    keep = []\n",
    "    while boxes:\n",
    "        current = boxes.pop(0)\n",
    "        keep.append(current)\n",
    "        boxes = [b for b in boxes if box_iou(current, b) < iou_thresh]\n",
    "    return keep\n",
    "\n",
    "\n",
    "def box_iou(box_a, box_b):\n",
    "    xa1, ya1 = box_a[2], box_a[3]\n",
    "    xa2, ya2 = xa1 + box_a[4], ya1 + box_a[5]\n",
    "    xb1, yb1 = box_b[2], box_b[3]\n",
    "    xb2, yb2 = xb1 + box_b[4], yb1 + box_b[5]\n",
    "    inter_x1 = max(xa1, xb1)\n",
    "    inter_y1 = max(ya1, yb1)\n",
    "    inter_x2 = min(xa2, xb2)\n",
    "    inter_y2 = min(ya2, yb2)\n",
    "    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "    area_a = box_a[4] * box_a[5]\n",
    "    area_b = box_b[4] * box_b[5]\n",
    "    union = area_a + area_b - inter_area + 1e-6\n",
    "    return inter_area / union\n",
    "\n",
    "\n",
    "def probmap_to_boxes(prob_map, min_area_ratio=1e-4, threshold=0.45, min_score=0.3, nms_thresh=0.4):\n",
    "    boxes = []\n",
    "    channels, h, w = prob_map.shape\n",
    "    for cls_idx in range(channels):\n",
    "        cls_map = prob_map[cls_idx]\n",
    "        heat = cv2.GaussianBlur(cls_map, (5, 5), 0)\n",
    "        mask = (heat >= threshold).astype(np.uint8)\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, bw, bh = cv2.boundingRect(cnt)\n",
    "            if bw * bh < min_area_ratio * h * w:\n",
    "                continue\n",
    "            score = float(heat[y:y+bh, x:x+bw].mean())\n",
    "            if score < min_score:\n",
    "                continue\n",
    "            boxes.append([cls_idx, score, x, y, bw, bh])\n",
    "    boxes = non_max_suppression(boxes, iou_thresh=nms_thresh)\n",
    "    normalized = []\n",
    "    for cls_idx, score, x, y, bw, bh in boxes:\n",
    "        cx = (x + bw / 2) / w\n",
    "        cy = (y + bh / 2) / h\n",
    "        nw = bw / w\n",
    "        nh = bh / h\n",
    "        normalized.append((cls_idx, score, cx, cy, nw, nh))\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def write_prediction_file(image_id, boxes, output_dir=RESULT_DIR):\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = output_dir / f'{image_id}.txt'\n",
    "    with path.open('w') as f:\n",
    "        for cls, score, cx, cy, w, h in boxes:\n",
    "            f.write(f'{cls} {cx:.6f} {cy:.6f} {w:.6f} {h:.6f}\\n')\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01da33e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9832390ab83c4d799d805531d1e2620f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result files: 120\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 推理 test 并写入120个结果文件\n",
    "for old in RESULT_DIR.glob('*.txt'):\n",
    "    old.unlink()\n",
    "\n",
    "test_paths = sorted(TEST_IMG_DIR.glob('*.jpg'))\n",
    "test_dataset = LeakInferenceDataset(test_paths)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "def _to_list(value):\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.tolist()\n",
    "    if isinstance(value, np.ndarray):\n",
    "        return value.tolist()\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return list(value)\n",
    "    return [value]\n",
    "\n",
    "\n",
    "def normalize_original_sizes(batch_sizes):\n",
    "    if isinstance(batch_sizes, torch.Tensor):\n",
    "        values = _to_list(batch_sizes)\n",
    "        if isinstance(values[0], (list, tuple)):\n",
    "            return [(int(h), int(w)) for h, w in values]\n",
    "        raise ValueError(f'unexpected tensor shape for original size: {batch_sizes.shape}')\n",
    "\n",
    "    if isinstance(batch_sizes, (list, tuple)):\n",
    "        if len(batch_sizes) == 2 and all(len(_to_list(bs)) == len(_to_list(batch_sizes[0])) for bs in batch_sizes):\n",
    "            heights = _to_list(batch_sizes[0])\n",
    "            widths = _to_list(batch_sizes[1])\n",
    "            return [(int(h), int(w)) for h, w in zip(heights, widths)]\n",
    "        normalized = []\n",
    "        for size in batch_sizes:\n",
    "            values = _to_list(size)\n",
    "            if len(values) == 1 and isinstance(values[0], (list, tuple)):\n",
    "                values = values[0]\n",
    "            if len(values) != 2:\n",
    "                raise ValueError(f'unexpected original size entry: {values}')\n",
    "            normalized.append((int(values[0]), int(values[1])))\n",
    "        return normalized\n",
    "\n",
    "    values = _to_list(batch_sizes)\n",
    "    if len(values) != 2:\n",
    "        raise ValueError(f'unexpected original size: {values}')\n",
    "    return [(int(values[0]), int(values[1]))]\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "        images, image_ids, original_sizes = batch\n",
    "        if isinstance(image_ids, str):\n",
    "            image_ids = [image_ids]\n",
    "        else:\n",
    "            image_ids = list(image_ids)\n",
    "        original_sizes = normalize_original_sizes(original_sizes)\n",
    "        images = images.to(DEVICE)\n",
    "        logits, _ = model(images)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        for prob, image_id, (orig_h, orig_w) in zip(probs, image_ids, original_sizes):\n",
    "            resized_map = np.stack([\n",
    "                cv2.resize(prob[c], (orig_w, orig_h), interpolation=cv2.INTER_LINEAR)\n",
    "                for c in range(prob.shape[0])\n",
    "            ])\n",
    "            boxes = probmap_to_boxes(resized_map)\n",
    "            write_prediction_file(image_id, boxes)\n",
    "\n",
    "print('result files:', len(list(RESULT_DIR.glob('*.txt'))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11acc054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0014.txt\n",
      "1 0.541146 0.069444 0.083333 0.118519\n",
      "1 0.360677 0.143519 0.080729 0.159259\n",
      "1 0.635417 0.141667 0.036458 0.074074\n",
      "2 0.540365 0.081481 0.053646 0.003704\n",
      "2 0.538281 0.088426 0.050521 0.004630\n",
      "2 0.547917 0.067593 0.046875 0.003704\n",
      "2 0.531771 0.102315 0.045833 0.004630\n",
      "2 0.544010 0.074537 0.053646 0.004630\n",
      "2 0.550260 0.060648 0.042188 0.002778\n",
      "2 0.534115 0.095833 0.050521 0.004630\n",
      "2 0.360156 0.143981 0.038021 0.002778\n",
      "2 0.530208 0.109722 0.037500 0.004630\n",
      "2 0.371615 0.200000 0.031771 0.003704\n",
      "2 0.371094 0.192593 0.039062 0.003704\n",
      "2 0.366667 0.178704 0.042708 0.003704\n",
      "2 0.364063 0.158333 0.038542 0.003704\n",
      "2 0.364844 0.172222 0.046354 0.003704\n",
      "2 0.366146 0.164815 0.034375 0.003704\n",
      "2 0.362240 0.150926 0.033854 0.003704\n",
      "2 0.368229 0.185648 0.044792 0.004630\n",
      "0 0.481250 0.420833 0.043750 0.145370\n",
      "0 0.972917 0.589815 0.050000 0.064815\n",
      "0 0.768750 0.125463 0.030208 0.073148\n",
      "0 0.138802 0.960648 0.077604 0.073148\n",
      "0 0.736458 0.186574 0.045833 0.136111\n",
      "0 0.733333 0.515741 0.025000 0.109259\n",
      "0 0.124479 0.776852 0.011458 0.033333\n",
      "0 0.753385 0.022685 0.021354 0.028704\n",
      "0 0.961198 0.668519 0.030729 0.031481\n",
      "0 0.788281 0.706019 0.027604 0.030556\n",
      "1 0.729167 0.213426 0.010417 0.012037\n",
      "0 0.020573 0.251852 0.030729 0.014815\n",
      "0 0.771094 0.641204 0.018229 0.039815\n",
      "1 0.469792 0.416667 0.021875 0.112963\n",
      "0 0.765885 0.462963 0.026562 0.029630\n",
      "1 0.984635 0.915741 0.010937 0.016667\n",
      "0 0.726042 0.399537 0.008333 0.025000\n",
      "0 0.777083 0.187500 0.011458 0.032407\n",
      "0 0.019792 0.184259 0.031250 0.085185\n",
      "0 0.983594 0.923148 0.023438 0.033333\n",
      "0 0.827344 0.767593 0.040104 0.053704\n",
      "0 0.108854 0.124537 0.125000 0.236111\n",
      "0 0.042188 0.881944 0.077083 0.230556\n",
      "0038.txt\n",
      "1 0.598177 0.518056 0.127604 0.130556\n",
      "1 0.373177 0.468981 0.068229 0.132407\n",
      "2 0.596875 0.525926 0.113542 0.003704\n",
      "2 0.599740 0.546759 0.099479 0.004630\n",
      "2 0.597135 0.532870 0.113021 0.004630\n",
      "2 0.598698 0.540278 0.107813 0.004630\n",
      "2 0.599479 0.519444 0.102083 0.003704\n",
      "2 0.373958 0.498148 0.041667 0.003704\n",
      "2 0.376302 0.484259 0.039062 0.003704\n",
      "2 0.601302 0.554167 0.093229 0.004630\n",
      "2 0.374740 0.477778 0.043229 0.003704\n",
      "2 0.626823 0.567593 0.027604 0.003704\n",
      "2 0.370312 0.512037 0.034375 0.003704\n",
      "2 0.373958 0.491204 0.042708 0.004630\n",
      "2 0.370052 0.505093 0.042188 0.004630\n",
      "2 0.609375 0.560648 0.071875 0.004630\n",
      "2 0.377865 0.470370 0.034896 0.003704\n",
      "0 0.462240 0.771296 0.046354 0.051852\n",
      "2 0.601823 0.512037 0.082812 0.003704\n",
      "1 0.466146 0.756019 0.027083 0.023148\n",
      "0 0.798177 0.141667 0.014063 0.033333\n",
      "0 0.493750 0.087500 0.053125 0.026852\n",
      "0 0.778646 0.143981 0.011458 0.036111\n",
      "0 0.795052 0.093519 0.014063 0.022222\n",
      "0 0.030729 0.441204 0.052083 0.063889\n",
      "0093.txt\n",
      "0 0.626823 0.915278 0.019271 0.045370\n",
      "0 0.889323 0.920833 0.013021 0.019444\n",
      "0154.txt\n",
      "1 0.481250 0.467593 0.065625 0.170370\n",
      "2 0.485938 0.477778 0.039583 0.003704\n",
      "2 0.483854 0.498148 0.035417 0.003704\n",
      "2 0.483333 0.512037 0.034375 0.003704\n",
      "2 0.481771 0.519444 0.037500 0.003704\n",
      "2 0.479427 0.505556 0.043229 0.003704\n",
      "2 0.485677 0.491667 0.039062 0.003704\n",
      "2 0.485938 0.484259 0.038542 0.003704\n",
      "2 0.487500 0.470370 0.035417 0.003704\n",
      "2 0.482031 0.525926 0.029687 0.003704\n",
      "2 0.486198 0.463889 0.039062 0.003704\n",
      "0 0.481510 0.755556 0.040104 0.042593\n",
      "2 0.485417 0.456481 0.030208 0.003704\n",
      "0 0.437240 0.192130 0.021354 0.030556\n",
      "0 0.347135 0.591667 0.027604 0.055556\n",
      "1 0.472135 0.249537 0.013021 0.036111\n",
      "0 0.386458 0.579167 0.018750 0.036111\n",
      "1 0.678125 0.648148 0.020833 0.033333\n",
      "0 0.847917 0.202315 0.008333 0.012037\n",
      "0 0.432812 0.406481 0.011458 0.014815\n",
      "0 0.260937 0.284722 0.017708 0.015741\n",
      "0 0.474219 0.237963 0.016146 0.020370\n",
      "0 0.281771 0.310648 0.021875 0.041667\n",
      "0 0.863542 0.158796 0.008333 0.021296\n",
      "0 0.472917 0.269444 0.012500 0.011111\n",
      "0 0.644271 0.147222 0.012500 0.044444\n",
      "0 0.880990 0.215741 0.029687 0.061111\n",
      "0194.txt\n",
      "1 0.535677 0.071296 0.093229 0.111111\n",
      "1 0.445573 0.650463 0.063021 0.112037\n",
      "1 0.629687 0.070370 0.070833 0.087037\n",
      "1 0.634115 0.604167 0.113021 0.091667\n",
      "1 0.522396 0.401852 0.059375 0.062963\n",
      "2 0.537760 0.081481 0.057813 0.003704\n",
      "2 0.536198 0.075000 0.053646 0.003704\n",
      "2 0.532552 0.088426 0.069271 0.004630\n",
      "2 0.450260 0.664815 0.030729 0.003704\n",
      "2 0.532813 0.095833 0.061458 0.004630\n",
      "2 0.532292 0.102315 0.053125 0.004630\n",
      "2 0.635417 0.081481 0.034375 0.003704\n",
      "2 0.627865 0.095370 0.043229 0.003704\n",
      "2 0.643229 0.623148 0.057292 0.003704\n",
      "2 0.444531 0.679167 0.041146 0.004630\n",
      "2 0.446354 0.671759 0.038542 0.004630\n",
      "2 0.445833 0.685648 0.036458 0.004630\n",
      "2 0.629687 0.630093 0.068750 0.004630\n",
      "2 0.450781 0.658333 0.030729 0.003704\n",
      "2 0.627865 0.088426 0.049479 0.004630\n",
      "2 0.530729 0.109722 0.039583 0.004630\n",
      "2 0.445833 0.692593 0.027083 0.003704\n",
      "2 0.630208 0.102778 0.034375 0.003704\n",
      "2 0.634115 0.616667 0.038021 0.003704\n",
      "2 0.538021 0.067593 0.034375 0.003704\n",
      "2 0.624479 0.637500 0.057292 0.004630\n",
      "2 0.521094 0.422222 0.027604 0.003704\n",
      "0 0.511979 0.600926 0.036458 0.038889\n",
      "0 0.639323 0.739815 0.035937 0.038889\n",
      "2 0.518750 0.415278 0.033333 0.004630\n",
      "0 0.751563 0.062500 0.015625 0.023148\n",
      "0 0.396875 0.561574 0.009375 0.017593\n"
     ]
    }
   ],
   "source": [
    "sample_files = sorted(RESULT_DIR.glob('*.txt'))[:5]\n",
    "for path in sample_files:\n",
    "    print(path.name)\n",
    "    with path.open() as f:\n",
    "        print(f.read().strip())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}